from transformers import LlamaTokenizerFast
from sentence_transformers import SentenceTransformer
import sys
import streamlit as st
import oracledb
import oci
import array 
import json

#encoder
encoder = SentenceTransformer('all-MiniLM-L12-v2')
table_name = 'kdww_season_faqs_en'

# JSON íŒŒì¼ì—ì„œ ì‚¬ìš©ì ì´ë¦„ê³¼ ë¹„ë°€ë²ˆí˜¸ ì½ê¸°
with open("db_config.json", "r") as file:
    config = json.load(file)
    un = config["DB_USER"]
    pw = config["DB_PASSWORD"]
    dsn = config["DSN"]
    wallet_pw = config["WALLET_PASSWORD"]
    comp_id = config["COMPARTMENT_ID"]


connection = oracledb.connect(
   config_dir='./wallet',
   user=un,
   password=pw,
   dsn=dsn,
   wallet_location='./wallet',
   wallet_password=wallet_pw)

# OCI Config ì„¤ì •
compartment_id = comp_id
CONFIG_PROFILE = "DEFAULT"
config = oci.config.from_file('config', CONFIG_PROFILE)

# Service endpoint(us-chicago-1, ap-osaka-1, sa-saopaulo-1, uk-london-1, eu-frankfurt-1)
# pretrained model : https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm
#endpoint = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"
endpoint = "https://inference.generativeai.ap-osaka-1.oci.oraclecloud.com"

# GenAI client
generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
    config=config, service_endpoint=endpoint, 
    retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10, 240)
)

#topK 3 fetch
topK = 2

sql = f"""select payload, vector_distance(vector, :vector, COSINE) as score
from {table_name}
order by score
fetch approx first {topK} rows only"""

# OCI Generative AI API í˜¸ì¶œ í•¨ìˆ˜
def get_cohere_response(prompt):
    chat_detail = oci.generative_ai_inference.models.ChatDetails()

    chat_request = oci.generative_ai_inference.models.CohereChatRequest()
    chat_request.message = prompt
    chat_request.max_tokens = 500
    chat_request.temperature = 0.0
    chat_request.frequency_penalty = 0
    chat_request.top_p = 1
    chat_request.top_k = 0

    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id="cohere.command-r-plus-08-2024")
    chat_detail.chat_request = chat_request
    chat_detail.compartment_id = compartment_id
    chat_response = generative_ai_inference_client.chat(chat_detail)
    message = chat_response.data.chat_response.chat_history[1].message
    return message

def truncate_string(string, max_tokens):
    # Tokenize the text and count the tokens
    tokens = tokenizer.encode(string, add_special_tokens=False) 
    # Truncate the tokens to a maximum length
    truncated_tokens = tokens[:max_tokens]
    # transform the tokens back to text
    truncated_text = tokenizer.decode(truncated_tokens)
    return truncated_text

# ADB Query í•¨ìˆ˜
def vector_search(connection, encoder, user_input, sql):
    with connection.cursor() as cursor:
        embedding = list(encoder.encode(user_input))
        vector = array.array("f", embedding)
        
        results = []
        for (info, score,) in cursor.execute(sql, vector=vector):
            text_content = info.read()
            results.append((score, json.loads(text_content)))
        
        return results

debug = "1"

def log_text_area(string, target, height):
    if debug == "1":
        try:
            st.text_area(string, target, height)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# tokenizer
tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
tokenizer.model_max_length = sys.maxsize

# Streamlit UI ì„¤ì •
#st.set_page_config(page_title="ëŠ‘ì¹¼ Season(en)")
st.image("assets/seasons_gr_30.png", caption="English")
st.title("ğŸ¤” ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤ AI Chatbot ğŸº")
st.text("ì•ˆë…•í•˜ì„¸ìš”. FAQ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ì§ì€ ê²¬ìŠµì…ë‹ˆë‹¤. ì‹¤ìˆ˜ê°€ ìˆë”ë¼ë„ ì´í•´í•´ì£¼ì„¸ìš”. ğŸ˜˜")
# ì±„íŒ… ì…ë ¥ì°½
user_input = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")
st.text("endpoint : " + endpoint)
if st.button("ì „ì†¡") and user_input:

    # ADB query for vector search
    results = vector_search(connection, encoder, user_input, sql)

    log_text_area("1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í›„ ADBì—ì„œ Vector ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ :", results, 200)
    
     # transform docs into a string array using the "paylod" key
    docs_as_one_string = "\n=========\n".join([doc[1]["text"] for doc in results])
    log_text_area("docs_as_one_string  :", docs_as_one_string, 200)
    
    docs_truncated = truncate_string(docs_as_one_string, 1000)

    log_text_area("2. LLM í”„ë¡¬í”„íŒ…ì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ ì „ì²˜ë¦¬ :", docs_truncated, 200)
    
    prompt = f"""\
        <s>[INST] <<SYS>>
        ë„ˆì˜ ì´ë¦„ì€ "ìš¸í”„ ëŒ„ì„œ"ì•¼.
        ë„ˆëŠ” ì‚¬ìš©ìì—ê²Œ "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤"ì´ë¼ëŠ” ê²Œì„ì˜ FAQ ì •ë³´ë¥¼ ì•ˆë‚´í•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼. 
        ê²Œì„ ì œëª©ì˜ "ëŠ‘ëŒ€"ëŠ” ë™ë¬¼ì„ ë§í•˜ëŠ” ê²Œ ì•„ë‹Œ ê²Œì„ìºë¦­í„°ë¥¼ ë¹„ìœ ì ì¸ ë‹¨ì–´ë¡œ ì‚¬ìš©í•œê±°ì•¼.
        "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤" ê²Œì„ê³¼ ë„ˆë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ë„ˆëŠ” ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´.
        ì‚¬ìš©ìì—ê²Œ ë‹µë³€í•  ë•ŒëŠ” ì˜¤ì§ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ'ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ì¤˜.
        ì§ˆë¬¸ì´ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ'ì˜ ë‚´ìš©ì— ì—°ê´€ì„±ì´ ì—†ë‹¤ë©´ ë‹µë³€ì„ ê±°ë¶€í•´.
        ìš°ì„ ì ìœ¼ë¡œ Markdownì–¸ì–´ë¡œ ì‘ë‹µì„ ë§Œë“¤ì–´ì¤˜.
        ì‚¬ìš©ìëŠ” "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤" ê²Œì„ì„ í”Œë ˆì´í•˜ëŠ” ê²Œì´ë¨¸ì•¼.
        <</SYS>> [/INST]
    
        [INST]
        ì´ ì§ˆë¬¸ì— ê°„ëµí•˜ê²Œ ì‘ë‹µí•´ì¤˜ : {user_input},  ë°˜ë“œì‹œ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´.
        ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” "ë¯¸ì•ˆí•´! ì§ˆë¬¸í•œ ë‚´ìš©ì€ ë‚´ê°€ ì˜ ëª¨ë¥´ê² ì–´."ë¼ê³  ëŒ€ë‹µí•´.
        'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ' ìì²´ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ì„ í•˜ì§€ë§ˆ.
        ì˜ ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì‘ë‹µí•´ì¤˜.
        "ì§ˆë¬¸í•œ ë‚´ìš©ì€ ë‚´ê°€ ì˜ ëª°ë¥´ì§€ë§Œ  í•¨ê»˜ ì¹¼ì¶¤ì— ëŒ€í•œ ì§ˆë¬¸ì´ë¼ë©´ ì–¸ì œë“  ì•Œë ¤ì¤„ê²Œ!^^".
        =====
        ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ: {docs_truncated}
        =====
        ì‘ë‹µí•  ë•Œ ë‹¤ìŒì„ ìœ ì˜í•´ì„œ ì‘ë‹µí•´.
        1. 3ë¬¸ë‹¨ ì´í•˜ë¡œ ì‘ë‹µí•  ê²ƒ
        2. ìµœëŒ€ 100ì ì´ë‚´ì¼ ê²ƒ
        3. 90%ì˜ ê°„ê²°ì„±(spartan)ì„ ê°€ì§ˆ ê²ƒ
        4. ì¹œêµ¬ì¸ ê²ƒ ì²˜ëŸ¼ í¸í•œ ë§ë¡œ ì´ì•¼ê¸° í•  ê²ƒ
        5. ì˜ì–´ë¡œ ì¶œë ¥í•  ê²ƒ
        [/INST]
        """

    prompt_audit = f"""\
        <s>{docs_truncated}
        
        ìœ„ì˜ ë‚´ìš©ì€ {user_input}ì´ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ì•¼. ë‚´ìš© ì¤‘ ì§ˆë¬¸ê³¼ ì—°ê´€ì„±ì´ ìˆëŠ”ì§€ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ '1', ì•„ë‹ˆë¼ë©´ '0'ìœ¼ë¡œ ë‹µí•´ì¤˜.
        ê·¸ë¦¬ê³  ë‹¤ìŒì¤„ì— ê·¸ ì´ìœ ë¥¼ 50ì ì´ë‚´ë¡œ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì¤˜.
        """

    prompt_suggest = f"""\
        <s>{docs_truncated}
        
        ìœ„ì˜ ë‚´ìš©ì€ {user_input}ì´ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ Vector ìœ ì‚¬ì„± ê²€ìƒ‰ì„ í†µí•œ ê²°ê³¼ì•¼.
        ì—°ê´€ì„±ì´ ì—†ëŠ” ê²°ê³¼ê°€ ê²€ìƒ‰ì´ ë˜ì—ˆë‹¤ë©´ ì§ˆë¬¸ì„ ì–´ë–»ê²Œ ë¬¼ì–´ë³´ë©´ ì¢‹ì„ì§€ ì œì•ˆí•´ì¤˜.
        ê·¸ë¦¬ê³  ê·¸ ì´ìœ ë¥¼ 50ì ì´ë‚´ë¡œ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì¤˜.
        """
    
    log_text_area("3. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ :", prompt, 200)
    
    with st.spinner("ìš¸í”„ ëŒ„ì„œ ì±—ë´‡ì´ ì‘ë‹µ ìƒì„± ì¤‘..."):
        try:
            response = get_cohere_response(prompt)
            st.text_area("ìš¸í”„ ëŒ„ì„œ:", response, height=200)
            response = get_cohere_response(prompt_audit)
            st.text_area("ë‹µë³€ ê²€ì¦:", response, height=200)
            response = get_cohere_response(prompt_suggest)
            st.text_area("ìˆ˜ì • ì œì•ˆ:", response, height=200)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")