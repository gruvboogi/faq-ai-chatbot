from transformers import LlamaTokenizerFast
from sentence_transformers import SentenceTransformer
import sys
import streamlit as st
import oracledb
import oci
import array 
import json

#encoder
encoder = SentenceTransformer('jhgan/ko-sroberta-multitask')
table_name = 'marketing_report_narative'

# JSON íŒŒì¼ì—ì„œ ì‚¬ìš©ì ì´ë¦„ê³¼ ë¹„ë°€ë²ˆí˜¸ ì½ê¸°
with open("db_config.json", "r") as file:
    config = json.load(file)
    un = config["DB_USER"]
    pw = config["DB_PASSWORD"]
    dsn = config["DSN"]
    wallet_pw = config["WALLET_PASSWORD"]
    comp_id = config["COMPARTMENT_ID"]


connection = oracledb.connect(
   config_dir='./wallet',
   user=un,
   password=pw,
   dsn=dsn,
   wallet_location='./wallet',
   wallet_password=wallet_pw)

# OCI Config ì„¤ì •
compartment_id = comp_id
CONFIG_PROFILE = "DEFAULT"
config = oci.config.from_file('config', CONFIG_PROFILE)

# Service endpoint(us-chicago-1, ap-osaka-1, sa-saopaulo-1, uk-london-1, eu-frankfurt-1)
# pretrained model : https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm
#endpoint = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"
endpoint = "https://inference.generativeai.ap-osaka-1.oci.oraclecloud.com"

# GenAI client
generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
    config=config, service_endpoint=endpoint, 
    retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10, 240)
)

#topK 3 fetch
topK = 2

sql = f"""select payload, vector_distance(vector, :vector, COSINE) as score
from {table_name}
order by score
fetch approx first {topK} rows only"""

# OCI Generative AI API í˜¸ì¶œ í•¨ìˆ˜
def get_cohere_response(prompt):
    chat_detail = oci.generative_ai_inference.models.ChatDetails()

    chat_request = oci.generative_ai_inference.models.CohereChatRequest()
    chat_request.message = prompt
    chat_request.max_tokens = 500
    chat_request.temperature = 0.0
    chat_request.frequency_penalty = 0
    chat_request.top_p = 1
    chat_request.top_k = 0

    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id="cohere.command-r-plus-08-2024")
    chat_detail.chat_request = chat_request
    chat_detail.compartment_id = compartment_id
    chat_response = generative_ai_inference_client.chat(chat_detail)
    message = chat_response.data.chat_response.chat_history[1].message
    return message

def truncate_string(string, max_tokens):
    # Tokenize the text and count the tokens
    tokens = tokenizer.encode(string, add_special_tokens=False) 
    # Truncate the tokens to a maximum length
    truncated_tokens = tokens[:max_tokens]
    # transform the tokens back to text
    truncated_text = tokenizer.decode(truncated_tokens)
    return truncated_text

# ADB Query í•¨ìˆ˜
def vector_search(connection, encoder, user_input, sql):
    with connection.cursor() as cursor:
        embedding = list(encoder.encode(user_input))
        vector = array.array("f", embedding)
        
        results = []
        for (info, score,) in cursor.execute(sql, vector=vector):
            text_content = info.read()
            results.append((score, json.loads(text_content)))
        
        return results

def select_ai(connection, select_ai_sql):
    with connection.cursor() as cursor:
        # Define the query to select all rows from a table
        query = f"SELECT AI {select_ai_sql}"
    
        # Execute the query
        cursor.execute(query)
    
        # Fetch all rows
        rows = cursor.fetchall()
    
        # Print the rows
        for row in rows[:1]:
            return row[0]

        
debug = "1"

def log_text_area(string, target, height):
    if debug == "1":
        try:
            st.text_area(string, target, height)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

with connection.cursor() as cursor:
    # set dbms_cloud_ai profile(already created)
    set_profile_sql = f"""
        BEGIN
            DBMS_CLOUD_AI.set_profile(profile_name => 'OCI_GENERATIVE_AI_PROFILE');
        END;
        """
    try:
        cursor.execute(set_profile_sql)
    except oracledb.DatabaseError as e:
        raise

    connection.autocommit = True

# tokenizer
tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
tokenizer.model_max_length = sys.maxsize

# Streamlit UI ì„¤ì •
#st.set_page_config(page_title="ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤ Season")
#st.image("assets/season_dalle.webp", caption="")
st.title("ğŸ¤” ë§ˆì¼€íŒ… ë³´ê³ ì„œ AI Chatbot ğŸº")
st.text("ì•ˆë…•í•˜ì„¸ìš”. FAQ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ì§ì€ ê²¬ìŠµì…ë‹ˆë‹¤. ì‹¤ìˆ˜ê°€ ìˆë”ë¼ë„ ì´í•´í•´ì£¼ì„¸ìš”. ğŸ˜˜")
st.text("example1 : 2023ë…„ ì´í›„ ê°€ì¥ ë§ì€ ì‹ ê·œ ìœ ì €ê°€ ìœ ì…ëœ ë§ˆì¼€íŒ… í™œë™ì„ ì°¾ì•„ì„œ ì „ì²´ ë‚´ìš©ì„ ì •ë¦¬í•´ì¤˜.")
st.text("example2 : 2023ë…„ ì´í›„ ê°€ì¥ ë§ì€ íƒˆí‡´ ìœ ì €ê°€ ë°œìƒí•œ ë§ˆì¼€íŒ… í™œë™ì„ ì°¾ì•„ì„œ ì „ì²´ ë‚´ìš©ì„ ì •ë¦¬í•´ì¤˜.")
# ì±„íŒ… ì…ë ¥ì°½
user_input = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?") + "(ê²°ê³¼ì— ë§ˆì¼€íŒ… ì§„í–‰ì¼ì, ë§ˆì¼€íŒ… ì˜ˆì‚°, ROI, ì£¼ìš” ê²°ê³¼ KPI ë° ì£¼ìš” ë§ˆì¼€íŒ… ì „ëµì„ í¬í•¨)"
st.text("endpoint : " + endpoint)
if st.button("ì „ì†¡") and user_input:

    # ADB query for vector search
    results = vector_search(connection, encoder, user_input, sql)

    select_ai_sql = f"""narrate {user_input}"""
    
    results_select_ai = select_ai(connection, select_ai_sql)

    log_text_area("1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í›„ ADBì—ì„œ Vector ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ :", results, 200)
    
     # transform docs into a string array using the "paylod" key
    docs_as_one_string = "\n=========\n".join([doc[1]["text"] for doc in results])
    log_text_area("docs_as_one_string  :", docs_as_one_string, 200)
    
    docs_truncated = truncate_string(docs_as_one_string, 1000)

    log_text_area("2. LLM í”„ë¡¬í”„íŒ…ì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ ì „ì²˜ë¦¬ :", docs_truncated, 200)
    
    prompt = f"""\
        <s>[INST] <<SYS>>
        ë„ˆì˜ ì´ë¦„ì€ "ìš¸í”„ ë§ˆì¼€í„°"ì•¼.
        ë„ˆëŠ” ì‚¬ìš©ìì—ê²Œ "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤"ì´ë¼ëŠ” ê²Œì„ì˜ ë§ˆì¼€íŒ… ë³´ê³ ì„œ ì •ë³´ë¥¼ ì•ˆë‚´í•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼. 
        ì‚¬ìš©ìì—ê²Œ ë‹µë³€í•  ë•ŒëŠ” ì˜¤ì§ 'vector_search_results'ë‚˜ 'select_ai_results' ì¤‘ í•˜ë‚˜ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ì¤˜.
        'vector_search_results', 'select_ai_results'ì˜ ë‚´ìš© ì¤‘ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‚´ìš©ì„ í•œ ê°€ì§€ë§Œ ì‚¬ìš©í•´.
        ì§ˆë¬¸ì´ 'vector_search_results', 'select_ai_results'ì˜ ë‚´ìš©ê³¼ ì—°ê´€ì„±ì´ ì—†ë‹¤ë©´ ë‹µë³€ì„ ê±°ë¶€í•´.
        ìš°ì„ ì ìœ¼ë¡œ Markdownì–¸ì–´ë¡œ ì‘ë‹µì„ ë§Œë“¤ì–´ì¤˜.
        ì‚¬ìš©ìëŠ” ê²Œì„ ë§ˆì¼€íŒ… ë¶€ì„œì˜ ë‹´ë‹¹ìë“¤ì´ì•¼.
        <</SYS>>[/INST]

        [INST]
        ì´ ì§ˆë¬¸ì— ê°„ëµí•˜ê²Œ ì‘ë‹µí•´ì¤˜ : {user_input},  ë°˜ë“œì‹œ 'vector_search_results', 'select_ai_results' ì¤‘ ê°€ì¥ ì ì ˆí•œ í•œ ê°€ì§€ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´.
        'vector_search_results', 'select_ai_results' ìì²´ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ì„ í•˜ì§€ë§ˆ.
        ì˜ ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì‘ë‹µí•´ì¤˜.
        "ì§ˆë¬¸í•œ ë‚´ìš©ì€ ì œê°€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ã…œ.ã…œ;;".
        =====
        vector_search_results: {docs_truncated}
        select_ai_results : {results_select_ai}
        =====
        ì‘ë‹µí•  ë•Œ ë‹¤ìŒì„ ìœ ì˜í•´ì„œ ì‘ë‹µí•´.
        1. 3ë¬¸ë‹¨ ì´í•˜ë¡œ ì‘ë‹µí•  ê²ƒ
        2. ë§ˆì¼€íŒ… ì§„í–‰ì¼ì, ë§ˆì¼€íŒ… ì˜ˆì‚°, ROI, ì£¼ìš” ê²°ê³¼ KPI ë° ì£¼ìš” ë§ˆì¼€íŒ… ì „ëµì„ ê°ê° ìµœëŒ€ 50ì ì´ë‚´ë¡œ ìš”ì•½ ì •ë¦¬í•  ê²ƒ
        3. 2ë²ˆì˜ ë‚´ìš© ì¤‘ì— í™•ì¸í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ 'í™•ì¸ í•„ìš”'ë¡œ ì‘ë‹µí•  ê²ƒ
        4. 90%ì˜ ê°„ê²°ì„±(spartan)ì„ ê°€ì§ˆ ê²ƒ
        5. ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì˜ ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•  ê²ƒ
        [/INST]
        """

    prompt_audit = f"""\
        <s>{docs_truncated}
        
        ìœ„ì˜ ë‚´ìš©ì€ {user_input}ì´ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ì•¼. ë‚´ìš© ì¤‘ ì§ˆë¬¸ê³¼ ì—°ê´€ì„±ì´ ìˆëŠ”ì§€ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ '1', ì•„ë‹ˆë¼ë©´ '0'ìœ¼ë¡œ ë‹µí•´ì¤˜.
        ê·¸ë¦¬ê³  ë‹¤ìŒì¤„ì— ê·¸ ì´ìœ ë¥¼ 50ì ì´ë‚´ë¡œ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì¤˜.
        """

    prompt_suggest = f"""\
        <s>{docs_truncated}
        
        ìœ„ì˜ ë‚´ìš©ì€ {user_input}ì´ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ Vector ìœ ì‚¬ì„± ê²€ìƒ‰ì„ í†µí•œ ê²°ê³¼ì•¼.
        ì—°ê´€ì„±ì´ ì—†ëŠ” ê²°ê³¼ê°€ ê²€ìƒ‰ì´ ë˜ì—ˆë‹¤ë©´ ì§ˆë¬¸ì„ ì–´ë–»ê²Œ ë¬¼ì–´ë³´ë©´ ì¢‹ì„ì§€ ì œì•ˆí•´ì¤˜.
        ê·¸ë¦¬ê³  ê·¸ ì´ìœ ë¥¼ 50ì ì´ë‚´ë¡œ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì¤˜.
        """
    
    log_text_area("3. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ :", prompt, 200)
    
    with st.spinner("ìš¸í”„ ë§ˆì¼€í„° ì±—ë´‡ì´ ì‘ë‹µ ìƒì„± ì¤‘..."):
        try:
            response = get_cohere_response(prompt)
            st.text_area("ìš¸í”„ ë§ˆì¼€í„°:", response, height=200)
            response = get_cohere_response(prompt_audit)
            st.text_area("ë‹µë³€ ê²€ì¦:", response, height=200)
            response = get_cohere_response(prompt_suggest)
            st.text_area("ìˆ˜ì • ì œì•ˆ:", response, height=200)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")