import json
import oracledb
import oci
import array 
import streamlit as st
from sentence_transformers import SentenceTransformer

# SentenceTransformerë¡œ ì¸ì½”ë” ì„¤ì • (í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ì í•©í•œ ëª¨ë¸ ì‚¬ìš©)
encoder = SentenceTransformer('jhgan/ko-sroberta-multitask')

# OCI connection ì„¤ì •
with open("db_config.json", "r") as file:
    config = json.load(file)
    un = config["DB_USER"]
    pw = config["DB_PASSWORD"]
    dsn = config["DSN"]
    wallet_pw = config["WALLET_PASSWORD"]
    comp_id = config["COMPARTMENT_ID"]

connection = oracledb.connect(
   config_dir='./wallet',
   user=un,
   password=pw,
   dsn=dsn,
   wallet_location='./wallet',
   wallet_password=wallet_pw)

# OCI Config ì„¤ì •
compartment_id = comp_id
CONFIG_PROFILE = "DEFAULT"
config = oci.config.from_file('config', CONFIG_PROFILE)

# Service endpoint ì„¤ì • (ìˆ˜ì •ë¨)
endpoint = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"

# GenAI client ì„¤ì •
generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
    config=config, service_endpoint=endpoint, 
    retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10, 240)
)

# Llama 70B ëª¨ë¸ ID ì„¤ì • (ìˆ˜ì •ë¨)
model_id = "meta.llama-3.1-70b-instruct"  # Llama 70B ëª¨ë¸ IDë¡œ ìˆ˜ì •

# ADB Query ì„¤ì • (vector search)
topK = 2
table_name = 'kdww_season_faqs_embv2'
sql = f"""select payload, vector_distance(vector, :vector, COSINE) as score
from {table_name}
order by score
fetch approx first {topK} rows only"""

# OCI Generative AI API í˜¸ì¶œ í•¨ìˆ˜
def get_llama_response(prompt):
    # API ìš”ì²­ ê°ì²´ ìƒì„±
    content = oci.generative_ai_inference.models.TextContent()
    content.text = prompt

    message = oci.generative_ai_inference.models.Message()
    message.role = "USER"
    message.content = [content]

    chat_request = oci.generative_ai_inference.models.GenericChatRequest()
    chat_request.api_format = oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC
    chat_request.messages = [message]
    chat_request.max_tokens = 600
    chat_request.temperature = 1
    chat_request.frequency_penalty = 0
    chat_request.presence_penalty = 0
    chat_request.top_p = 0.75
    chat_request.top_k = -1

    chat_detail = oci.generative_ai_inference.models.ChatDetails()
    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)  # Llama 70B ëª¨ë¸ ID
    chat_detail.chat_request = chat_request
    chat_detail.compartment_id = compartment_id

    # ì‹¤ì œ API ìš”ì²­ í˜¸ì¶œ
    chat_response = generative_ai_inference_client.chat(chat_detail)

    # ì‘ë‹µì—ì„œ ë©”ì‹œì§€ ë°˜í™˜
    return chat_response.data.chat_response.choices[0].message.content[0].text
    
# ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜
def vector_search(connection, user_input, sql):
    with connection.cursor() as cursor:
        # SentenceTransformerë¡œ ë²¡í„° ìƒì„±
        embedding = list(encoder.encode(user_input))
        vector = array.array("f", embedding)
        
        results = []
        for (info, score,) in cursor.execute(sql, vector=vector):
            text_content = info.read()
            results.append((score, json.loads(text_content)))
        
        return results

# Streamlit UI ì„¤ì •
st.image("assets/seasons_gr_30.png", caption="meta.llama-3.1-70b-instruct")
st.title("ğŸ¤” ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤ AI Chatbot ğŸº")
st.text("ì•ˆë…•í•˜ì„¸ìš”. FAQ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ì§ì€ ê²¬ìŠµì…ë‹ˆë‹¤. ì‹¤ìˆ˜ê°€ ìˆë”ë¼ë„ ì´í•´í•´ì£¼ì„¸ìš”. ğŸ˜˜")
user_input = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")
st.text("endpoint : " + endpoint)
if st.button("ì „ì†¡") and user_input:

    # ADB query for vector search
    results = vector_search(connection, user_input, sql)

    # docs_as_one_string ìƒì„±
    docs_as_one_string = "\n=========\n".join([doc[1]["text"] for doc in results])

    docs_truncated = docs_as_one_string[:1000]  # ì œí•œ ê¸¸ì´

    prompt = f"""\
        <s>[INST] <<SYS>>
        ë„ˆì˜ ì´ë¦„ì€ "ìš¸í”„ ëŒ„ì„œ"ì•¼.
        ë„ˆëŠ” ì‚¬ìš©ìì—ê²Œ "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤"ì´ë¼ëŠ” ê²Œì„ì˜ FAQ ì •ë³´ë¥¼ ì•ˆë‚´í•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼. 
        ê²Œì„ ì œëª©ì˜ "ëŠ‘ëŒ€"ëŠ” ë™ë¬¼ì„ ë§í•˜ëŠ” ê²Œ ì•„ë‹Œ ê²Œì„ìºë¦­í„°ë¥¼ ë¹„ìœ ì ì¸ ë‹¨ì–´ë¡œ ì‚¬ìš©í•œê±°ì•¼.
        "ëŠ‘ëŒ€ì™€ í•¨ê»˜ ì¹¼ì¶¤" ê²Œì„ê³¼ ë„ˆë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ë„ˆëŠ” ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´.
        ì‚¬ìš©ìì—ê²Œ ë‹µë³€í•  ë•ŒëŠ” ì˜¤ì§ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ'ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ì¤˜.
        ì§ˆë¬¸ì´ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ'ì˜ ë‚´ìš©ì— ì—°ê´€ì„±ì´ ì—†ë‹¤ë©´ ë‹µë³€ì„ ê±°ë¶€í•´.
        ìš°ì„ ì ìœ¼ë¡œ Markdownì–¸ì–´ë¡œ ì‘ë‹µì„ ë§Œë“¤ì–´ì¤˜.
        ì‚¬ìš©ìëŠ” "ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤" ê²Œì„ì„ í”Œë ˆì´í•˜ëŠ” ê²Œì´ë¨¸ì•¼.
        <</SYS>> [/INST]
    
        [INST]
        ì´ ì§ˆë¬¸ì— ê°„ëµí•˜ê²Œ ì‘ë‹µí•´ì¤˜ : {user_input},  ë°˜ë“œì‹œ 'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´.
        ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” "ë¯¸ì•ˆí•´! ì§ˆë¬¸í•œ ë‚´ìš©ì€ ë‚´ê°€ ì˜ ëª¨ë¥´ê² ì–´."ë¼ê³  ëŒ€ë‹µí•´.
        'ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ' ìì²´ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ì„ í•˜ì§€ë§ˆ.
        ì˜ ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì‘ë‹µí•´ì¤˜.
        "ì§ˆë¬¸í•œ ë‚´ìš©ì€ ë‚´ê°€ ì˜ ëª°ë¥´ì§€ë§Œ  í•¨ê»˜ ì¹¼ì¶¤ì— ëŒ€í•œ ì§ˆë¬¸ì´ë¼ë©´ ì–¸ì œë“  ì•Œë ¤ì¤„ê²Œ!^^".
        =====
        ëŠ‘ëŒ€ì™€í•¨ê»˜ì¹¼ì¶¤FAQ: {docs_truncated}
        =====
        ì‘ë‹µí•  ë•Œ ë‹¤ìŒì„ ìœ ì˜í•´ì„œ ì‘ë‹µí•´.
        1. 3ë¬¸ë‹¨ ì´í•˜ë¡œ ì‘ë‹µí•  ê²ƒ
        2. ìµœëŒ€ 100ì ì´ë‚´ì¼ ê²ƒ
        3. 90%ì˜ ê°„ê²°ì„±(spartan)ì„ ê°€ì§ˆ ê²ƒ
        4. ì¹œêµ¬ì¸ ê²ƒ ì²˜ëŸ¼ í¸í•œ ë§ë¡œ ì´ì•¼ê¸° í•  ê²ƒ
        [/INST]
        """

    with st.spinner("ìš¸í”„ ëŒ„ì„œ ì±—ë´‡ì´ ì‘ë‹µ ìƒì„± ì¤‘..."):
        try:
            # Llama 70B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = get_llama_response(prompt)
            st.text_area("ìš¸í”„ ëŒ„ì„œ:", response, height=200)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
